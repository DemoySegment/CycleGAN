{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64c4ceb-70de-47aa-9dbe-0df02422e87d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:39.429209Z",
     "iopub.status.busy": "2023-04-15T02:07:39.429209Z",
     "iopub.status.idle": "2023-04-15T02:07:40.244346Z",
     "shell.execute_reply": "2023-04-15T02:07:40.244346Z",
     "shell.execute_reply.started": "2023-04-15T02:07:39.429209Z"
    },
    "id": "f64c4ceb-70de-47aa-9dbe-0df02422e87d",
    "outputId": "aadb6ce0-087c-48e5-84ec-d831147c9b88",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-05-17T07:34:43.665161600Z",
     "start_time": "2023-05-17T07:34:43.617160200Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CP2ncU_95oTS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CP2ncU_95oTS",
    "outputId": "4e3ec824-ef1c-4afb-99e1-895d2c07ad52",
    "ExecuteTime": {
     "end_time": "2023-05-17T07:34:44.581287200Z",
     "start_time": "2023-05-17T07:34:44.545763400Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      2\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f5562-53dc-4399-b5fd-027673b08ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:40.245419Z",
     "iopub.status.busy": "2023-04-15T02:07:40.245419Z",
     "iopub.status.idle": "2023-04-15T02:07:40.259940Z",
     "shell.execute_reply": "2023-04-15T02:07:40.259940Z",
     "shell.execute_reply.started": "2023-04-15T02:07:40.245419Z"
    },
    "id": "a18f5562-53dc-4399-b5fd-027673b08ad7",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.374851900Z"
    }
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, train = True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        \n",
    "    def augment(self, image, flipCode):\n",
    "        # using flip as data augmentation\n",
    "        flip = cv2.flip(image, flipCode)\n",
    "        return flip\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        img = img.resize((128, 128))\n",
    "        mask = mask.resize((128, 128))\n",
    "\n",
    "        # convert the PIL Image into a numpy array\n",
    "        img = np.array(img)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        mask[mask>0] = 1\n",
    "        mask = mask\n",
    "        if self.train == True:\n",
    "            flipCode = random.choice([1, 2])\n",
    "            if flipCode != 2:\n",
    "                img = self.augment(img, flipCode)\n",
    "                mask = self.augment(mask, flipCode)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        img = torch.as_tensor(img, dtype=torch.uint8)\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b5579-bbec-42f2-aa64-d3c9b7c694ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:40.526328Z",
     "iopub.status.busy": "2023-04-15T02:07:40.526328Z",
     "iopub.status.idle": "2023-04-15T02:07:40.539901Z",
     "shell.execute_reply": "2023-04-15T02:07:40.539901Z",
     "shell.execute_reply.started": "2023-04-15T02:07:40.526328Z"
    },
    "id": "550b5579-bbec-42f2-aa64-d3c9b7c694ea",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.375852Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = PennFudanDataset('/content/drive/MyDrive/PennFudanPed')\n",
    "dataset2 = PennFudanDataset('/content/drive/MyDrive/PennFudanPed',False)\n",
    "\n",
    "# train test val split\n",
    "dataset = ConcatDataset([dataset,dataset2])\n",
    "train_index, test_index = train_test_split(range(len(dataset)), test_size=0.1, random_state=42)\n",
    "train_index, val_index = train_test_split(train_index, test_size=0.1, random_state=42)\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "val_sampler = SubsetRandomSampler(val_index)\n",
    "test_sampler = SubsetRandomSampler(test_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6107fa-1feb-499a-9bb8-0702be16c8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:40.762201Z",
     "iopub.status.busy": "2023-04-15T02:07:40.762201Z",
     "iopub.status.idle": "2023-04-15T02:07:40.777586Z",
     "shell.execute_reply": "2023-04-15T02:07:40.777586Z",
     "shell.execute_reply.started": "2023-04-15T02:07:40.762201Z"
    },
    "id": "cf6107fa-1feb-499a-9bb8-0702be16c8f8",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.378852400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8, sampler=train_sampler, num_workers=0)\n",
    "vaild_loader = torch.utils.data.DataLoader(dataset, batch_size=8, sampler=val_sampler, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=8, sampler=test_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5746c-ba59-49cc-9787-b99a0123eb0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:41.004263Z",
     "iopub.status.busy": "2023-04-15T02:07:41.004263Z",
     "iopub.status.idle": "2023-04-15T02:07:41.015778Z",
     "shell.execute_reply": "2023-04-15T02:07:41.015778Z",
     "shell.execute_reply.started": "2023-04-15T02:07:41.004263Z"
    },
    "id": "f9e5746c-ba59-49cc-9787-b99a0123eb0a",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.380851900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here I use DICE-Loss which is also called soft dice-loss as my loss function\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a27359-dee7-4f23-93fc-e6c11690c8e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:07:42.630190Z",
     "iopub.status.busy": "2023-04-15T02:07:42.629189Z",
     "iopub.status.idle": "2023-04-15T02:07:42.650069Z",
     "shell.execute_reply": "2023-04-15T02:07:42.650069Z",
     "shell.execute_reply.started": "2023-04-15T02:07:42.630190Z"
    },
    "id": "61a27359-dee7-4f23-93fc-e6c11690c8e0",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.383853800Z"
    }
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    # using upsample instead of transpose conv\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, out_ch=1):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "        \n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(in_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "       # self.active = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "        d5 = torch.cat((e4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((e3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((e2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((e1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        #d1 = self.active(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pXgi_1nQHLqz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXgi_1nQHLqz",
    "outputId": "fd2dfdbc-745f-4f54-d6a6-7557c276fafd",
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.386854700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = U_Net()\n",
    "summary(model,input_size = (8,3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53250697-c0cc-4ba7-952c-b26849e6e6ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-15T02:29:53.028435Z",
     "iopub.status.busy": "2023-04-15T02:29:53.028435Z",
     "iopub.status.idle": "2023-04-15T02:29:53.037697Z",
     "shell.execute_reply": "2023-04-15T02:29:53.037697Z",
     "shell.execute_reply.started": "2023-04-15T02:29:53.028435Z"
    },
    "id": "53250697-c0cc-4ba7-952c-b26849e6e6ba",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.388853300Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, train_loader, valid_loader, num_epochs, lr):\n",
    "    best_acc = 0\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # criterion = DiceLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_dices = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step(epoch)\n",
    "        lr = scheduler.get_lr()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            images = images.float()\n",
    "            masks =masks.float()\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(images)\n",
    "            # outputs = model(images)[:,1,:,:].unsqueeze(1)\n",
    "            loss = loss_func(pred, masks)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(f'Train loss: {train_loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_dice = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(valid_loader):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                images = images.float()\n",
    "                masks =masks.float()\n",
    "\n",
    "                pred = model(images)\n",
    "            # outputs = model(images)[:,1,:,:].unsqueeze(1)\n",
    "                loss = loss_func(pred, masks)\n",
    "                DICE_loss = loss_func(pred,masks)\n",
    "                valid_loss += loss.item() * images.size(0)\n",
    "                valid_dice += DICE_loss*images.size(0)\n",
    "\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "        valid_dice = valid_dice / len(valid_loader.dataset)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        valid_dice = 1-valid_loss\n",
    "        valid_dices.append(valid_dice)\n",
    "        \n",
    "        \n",
    "\n",
    "        if valid_dice >= best_acc:\n",
    "\n",
    "\n",
    "            best_acc = valid_dice\n",
    "\n",
    "            torch.save(model.state_dict(),'test{}.pth'.format(epoch))\n",
    "\n",
    "        print(f'Valid loss: {valid_loss:.4f} - Dice: {valid_dice:.4f}')\n",
    "\n",
    "    return train_losses, valid_losses, valid_dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dec54e-116b-4b72-86e4-8cedd478a845",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2023-04-15T02:29:54.143614Z",
     "iopub.status.busy": "2023-04-15T02:29:54.143614Z",
     "iopub.status.idle": "2023-04-15T02:31:06.217146Z",
     "shell.execute_reply": "2023-04-15T02:31:06.217146Z",
     "shell.execute_reply.started": "2023-04-15T02:29:54.143614Z"
    },
    "id": "d2dec54e-116b-4b72-86e4-8cedd478a845",
    "outputId": "0e2851a7-119a-4de9-b0b7-cb76cae34d63",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.391854600Z"
    }
   },
   "outputs": [],
   "source": [
    "model = U_Net()\n",
    "initial_lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr) # try SGD\n",
    "#opt = optim.SGD(model_test.parameters(), lr = initial_lr, momentum=0.99)\n",
    "loss_func = DiceLoss()\n",
    "MAX_STEP = int(1e10)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, MAX_STEP, eta_min=1e-5)\n",
    "train_losses,valid_losses,valid_dices=train(model, train_loader = train_loader, valid_loader = vaild_loader, num_epochs = 40, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46276f40-43f2-42b2-a8d1-1b0b4aae8fe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "execution": {
     "iopub.execute_input": "2023-04-15T02:36:11.633624Z",
     "iopub.status.busy": "2023-04-15T02:36:11.633624Z",
     "iopub.status.idle": "2023-04-15T02:36:11.749458Z",
     "shell.execute_reply": "2023-04-15T02:36:11.749458Z",
     "shell.execute_reply.started": "2023-04-15T02:36:11.633624Z"
    },
    "id": "46276f40-43f2-42b2-a8d1-1b0b4aae8fe4",
    "outputId": "5dc609aa-3d78-4372-af22-9e596dbf6e8f",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.394854500Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,40,40)\n",
    "plt.plot(x,train_losses,'s-',color = 'r',label=\"train_losses\")\n",
    "plt.plot(x,valid_losses,'o-',color = 'g',label=\"valid_losses\")\n",
    "plt.plot(x,valid_dices,'o-',color = 'b',label=\"valid_dice\")\n",
    "plt.xlabel(\"epoch number\")\n",
    "plt.ylabel(\"accuracy and loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HB9sgVWrJT_7",
   "metadata": {
    "id": "HB9sgVWrJT_7"
   },
   "source": [
    "Here we can see that the train loss keeps going down, while the valid_loss reduce smoothly during the first 10 epoch and starts to fluctuate later. We can also notice that the val_loss doesn't largly come down after 30 epoch, and even become a little larger, which indicates a overfitting after about 30 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0nI1z1l67zkl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0nI1z1l67zkl",
    "outputId": "28ac6b36-4a56-4a61-ef71-6186e035de61",
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.396852400Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = U_Net()\n",
    "net.load_state_dict(torch.load('/content/drive/MyDrive/test19.pth', map_location=device))\n",
    "net.to(device)\n",
    "net.eval()\n",
    "av_dice = 0\n",
    "test_dice = 0\n",
    "with torch.no_grad():\n",
    "  for images, masks in tqdm(test_loader):\n",
    "    images = images.to(device)\n",
    "    masks = masks.to(device)\n",
    "    images = images.float()\n",
    "    masks =masks.float() \n",
    "    pred = model(images)\n",
    "    DICE_loss = loss_func(pred,masks)\n",
    "    test_dice += DICE_loss*images.size(0)\n",
    "test_Dice = test_dice / len(test_loader.dataset)\n",
    "print('the average dice on testset is {}'.format(1-test_Dice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BjeijuNEcvC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "1BjeijuNEcvC",
    "outputId": "ba07e508-17bf-4ab6-b302-5839bd6ea77b",
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.398851700Z"
    }
   },
   "outputs": [],
   "source": [
    "image,masks = next(iter(train_loader))\n",
    "image = image.to(device)\n",
    "image = image.float()\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  predict = net(image)\n",
    "for i in range(3):\n",
    "  img = image[i,:,:,:]\n",
    "  pred = predict[i,:,:,:]\n",
    "  mask = masks[i,:,:]\n",
    "  img = img.squeeze().permute(1,2,0).cpu().numpy()\n",
    "  img = np.uint8(img)\n",
    "  pred = torch.sigmoid(pred.squeeze().cpu()).numpy()\n",
    "  mask = mask.squeeze().numpy()\n",
    "  pred = np.where(pred>=0.5,1,0)\n",
    "  plt.figure()\n",
    "  plt.subplot(1,3,1)\n",
    "  plt.imshow(img)\n",
    "  plt.title('the train image{}'.format(i))\n",
    "  plt.axis('off')\n",
    "  plt.subplot(1,3,2)\n",
    "  plt.imshow(mask,cmap='rainbow')\n",
    "  plt.title('the train mask{}'.format(i))\n",
    "  plt.axis('off')\n",
    "  plt.subplot(1,3,3)\n",
    "  plt.imshow(pred,cmap='rainbow')\n",
    "  plt.title('the predicted mask{}'.format(i))\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c632a60-fee8-4027-9ca6-c95799003fa7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "execution": {
     "iopub.execute_input": "2023-04-15T02:10:56.050663Z",
     "iopub.status.busy": "2023-04-15T02:10:56.050663Z",
     "iopub.status.idle": "2023-04-15T02:10:56.445302Z",
     "shell.execute_reply": "2023-04-15T02:10:56.445302Z",
     "shell.execute_reply.started": "2023-04-15T02:10:56.050663Z"
    },
    "id": "1c632a60-fee8-4027-9ca6-c95799003fa7",
    "outputId": "82764657-d37b-4b69-cbea-ce2c369edfd0",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.400853300Z"
    }
   },
   "outputs": [],
   "source": [
    "img,mask = next(iter(test_loader))\n",
    "img = img.to(device)\n",
    "img = img.float()\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  pred = net(img)\n",
    "img = img.squeeze().permute(1,2,0).cpu().numpy()\n",
    "img = np.uint8(img)\n",
    "pred = torch.sigmoid(pred.squeeze().cpu()).numpy()\n",
    "mask = mask.squeeze().numpy()\n",
    "pred = np.where(pred>=0.5,1,0)\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.title('the test image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(mask,cmap='rainbow')\n",
    "plt.title('the test mask')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pred,cmap='rainbow')\n",
    "plt.title('the predicted mask')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YfDUsj5LLhyT",
   "metadata": {
    "id": "YfDUsj5LLhyT"
   },
   "source": [
    "Here we can see that my model has done a good job of segmenting the entire contour shape and position of an image in the test set, except for the person's head， which indicates a quite good generaliztion capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e909804-9fd8-4104-9fe3-e90098653c55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "1e909804-9fd8-4104-9fe3-e90098653c55",
    "outputId": "b00b32ff-d620-4869-a5d4-e93ecee850d2",
    "ExecuteTime": {
     "start_time": "2023-05-17T07:34:36.402851400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  img = Image.open('/content/drive/MyDrive/bettles.jpg')\n",
    "  img = img.resize((128, 128))\n",
    "  img = np.array(img)          \n",
    "  img = np.transpose(img, (2, 0, 1))\n",
    "  img = torch.as_tensor(img, dtype=torch.uint8).unsqueeze(0)                                       \n",
    "  pred = net(img.to(device).float())\n",
    "img = img.squeeze().permute(1,2,0).cpu().numpy()\n",
    "img = np.uint8(img)                       \n",
    "pred = torch.sigmoid(pred.squeeze().cpu()).numpy()\n",
    "pred = np.where(pred>=0.5,1,0)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title('the out of set image')\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pred,cmap='rainbow')\n",
    "plt.title('the predicted mask')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cR3T7Q7mMYG1",
   "metadata": {
    "id": "cR3T7Q7mMYG1"
   },
   "source": [
    "My model is stll able to segment the bettles from the background, but the human shapes are not quite smooth and there are some extra part of the mask In conclusion, this model's generalization capability is not so excellent on a out of distribution data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
